Afin de répondre à l'aspect "boîte noire" du modèle utilisé, nous avons implémenté de l'interprétabilité dans notre modèle à l'aide de Grad-CAM~\cite{GRADCAM}. Cette méthode permet de fournir une explication visuelle vis-à-vis des décisions de classification issue de notre modèle, permettant ainsi de rajouter une certaine légitimité relative face à nos analyses de traces de sang faisant partie d'un processus judiciaire. 

L'algorithme Grad-CAM (Gradient-weighted Class Activation Mapping) est une technique utilisée pour rendre les réseaux de neurones convolutifs (CNN) plus interprétables dans le domaine de la classification.
Il fonctionne en générant des cartes de chaleur (heatmaps) qui mettent en évidence les zones importantes d'une image qui contribuent le plus à la prédiction d'une classe spécifique.
Pour ce faire, Grad-CAM calcule les dérivées des scores de sortie de la classe cible par rapport aux caractéristiques de la dernière couche convolutive du CNN.
Ces dérivées sont ensuite globalement moyennées pour obtenir les poids d'importance de chaque carte de caractéristiques.
Enfin, les cartes de caractéristiques sont pondérées par les poids d'importance et combinées pour obtenir la carte de chaleur Grad-CAM, qui peut être superposée à l'image d'origine pour une visualisation plus intuitive.
Cette approche permet de mieux comprendre les décisions prises par le CNN et d'améliorer la confiance dans les prédictions du modèle.
La Figure~\ref{fig:grad_cam_example} montre un exemple d'une image de trace de sang avec sa carte de chaleur Grad-CAM superposée.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.40\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../asset/exemple/14.jpg}
    \end{subfigure}
    \begin{subfigure}{0.40\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../asset/exemple/14_saliency.png}
    \end{subfigure}
    \caption{Exemple d'une image  de trace de sang (à gauche) avec sa carte de chaleur Grad CAM superposée (à droite).}
    \label{fig:grad_cam_example}
\end{figure}

Nous avons remarqué que parfois certaines des réglettes permettant de donner l'échelle de la photo étaient détectées comme des traces de sang ou induisaient un biais dans la classification.
En utilisant Grad-CAM, nous avons pu identifier que le modèle se focalisait parfois sur ces réglettes pour prendre sa décision, ce qui nous a permis de mieux comprendre les erreurs de classification, comme le montre la Figure~\ref{fig:grad_cam reglette}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.40\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../asset/exemple/attention_reglette.jpg}
    \end{subfigure}
    \begin{subfigure}{0.40\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../asset/exemple/attention_reglette_image.jpg}
    \end{subfigure}
    \caption{Exemple d'une image  de trace de sang (à droite) avec sa carte de chaleur Grad CAM superposée (à gauche), dans le cadre d'une attention portée à la réglette.}
    \label{fig:grad_cam reglette}
\end{figure}


% Un autre avantage de Grad-CAM est qu'il permet de vérifier si le modèle se focalise sur la bonne tâche de sang
% dans le cas où plusieurs tâches sont présentes sur une même image.

% Dans l'exemple suivant, nous voyons que c'est la tâche de gauche qui est détectée par le modèle, ce qui est confirmé par la carte de chaleur Grad-CAM qui met en évidence les zones importantes de l'image pour cette tâche.


De plus, nous avons également implémenté les métriques average drop, average gain et average increase, définie dans la partie~\ref{sec: grad metrics} (voir l'article~\cite{opticam}), qui consiste à multiplier l'image de départ par la carte de saillence obtenue avec Grad-CAM puis de redonner cette image au modèle
pour obtenir une nouvelle prédiction.
Cela permet en théorie d'éliminer les parties de l'image qui ne sont pas importantes pour la prédiction et de se focaliser sur les zones importantes, et 
donc d'améliorer la qualité des prédictions du modèle, de renforcer la confiance dans les prédictions et de mieux comprendre les décisions prises par le modèle.

Cela permet en théorie d'éliminer les parties de l'image qui ne sont pas importantes pour la prédiction et de se focaliser sur les zones importantes, et
donc d'améliorer la qualité des prédictions du modèle, de renforcer la confiance dans les prédictions et de mieux comprendre les décisions prises par le modèle.

Plus précisement, ces métriques sont utilisées pour mesurer l'impact du masquage sur les probabilités de classe prédites d'une image d'entrée. Plus précisément, l'image d'entrée est masquée avec une carte de saliency, et les probabilités de classe prédites pour les images originales et masquées sont comparées.
L'average drop (AD) est utilisée pour quantifier la perte de puissance prédictive lorsque l'image est masquée. Elle est définie comme la différence moyenne entre les probabilités prédites pour les images originales et masquées. Une valeur plus faible est préférable pour l'average drop.
Enfin, l'average increase (AI) est utilisée pour quantifier le changement global de la puissance prédictive lorsque l'image est masquée, elle est calculée comme le pourcentage des cas où le masquage a augmenté la probabilité prédite. 
Ces métriques sont utilisées pour évaluer les performances de différentes méthodes de masquage de l'image d'entrée, telles que Opticam, Fakecam, Gradcam et autres. En comparant les valeurs de ad, ag et ai pour différentes méthodes, on peut déterminer quelle méthode est la meilleure pour une tâche donnée.
Voici les résultats obtenus avec ces méthodes sur une image de trâce de sang:

\begin{table}[ht]
    \centering
    \begin{tabular}{cccc}
        \toprule
        Métriques & Average Drop & Average Increase & Average Gain \\
        \midrule
        AWL ResNet & 91.6 & 0.0& 0.0\\
        FT AWL ResNet & 87.6 & 0.0 & 0.0\\
        \bottomrule
        \end{tabular}
    \caption{Résultats des saliency maps sur les données réelles. (Lower is better)}
    \label{tab:saliency_results}
\end{table}
